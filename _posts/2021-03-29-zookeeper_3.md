---
layout: post
title: Zookeeper学习（3）
date: 2021-03-29
Author: Jack
categories: [Zookeeper]
tags: [zookeeper, PAXOS, Raft]
comments: true
---

本篇学习一下分布式共识‘Consensus’算法PAXOS, raft以及ZAB。

**PAXOS**

`Paxos`算法中角色有`Proposer`, `Acceptor`,`Learner`. Proposer负责发起Propose，说明他想选择的值，Acceptor负责投票，决定是否接受propose. Learner负责记录结果（非必须，也可以由Proposer记录结果。）

整个流程需要两个阶段

阶段一： 准备阶段

Proposer的行为：

在这个阶段，Proposer要向Acceptor申请投票，因此他会将一个PREPARE请求发送给超过半数/全部的Acceptor，这个PREPARE请求里面包含了一个unique的ID, 用来表示Proposal的先后顺序。为了要保证unique，可以用 round_number + server_id 组成一个unique的ID。 如果发送给Acceptor的消息没有回复，那么Proposer会自增round_number重新产生ID，然后重新发送。

Acceptor的行为：

当Acceptor收到Proposer的PREPARE请求之后，他首先要确定自己是否做出过承诺（承诺忽略一个Proposal ID小于给过承诺的Proposal ID， 从而使当前PREPARE请求timeout），为此，Acceptor要记录目前它接受过的最大的Proposal ID, 记为R。如果PREPARE包含的ID小于R，那么Acceptor会无视他。如果PREPARE包含的ID大于R，那么Acceptor会用新的ID更新自己的R值，然后根据自己是否已经Accept过Proposal做出以下选择：

* 如果已经Accept过Proposal，那么就返回R，已经接受的Proposal ID 和 value。（这就要求Acceptor还要保存最后接受的Proposal的ID和value）
* 如果还没有Accept过Proposal，那么就返回R。

阶段二： 请求阶段

Proposer的行为：

在这个阶段，Proposer在接收到超过半数的Acceptor返回的信息后，如果信息里面有Proposal ID和value，那么就从这些信息里面选出最大Proposal ID的value当做自己的value来用，然后向超过半数的Acceptor发起ACCEPT请求，这个请求要包含自己的Proposal ID和自己选择的Value。

Acceptor的行为：

当Acceptor收到Proposer的ACCEPT请求后，如果Acceptor的R小于等于Proposal ID的话，就会用Proposal ID来更新R，并且接受这个Proposal，还要把Proposal的ID和value存起来，并返回R给Proposer(返回R和value给Learner)。否则，那么无视掉这个Proposal。

Proposer/Learner的行为：

当Proposer/Learner收到超过半数的Acceptor的accept结果后，他就可以认定value被Accept，从而可以放心的交给状态机去处理value。

注意：

* Consensus只是针对Value, 各个Acceptor也只是对value有一致接受，而不包括Proposal ID。
* 【Livelock】由于proposal ID unique，如果有两个或多个Proposer同时在PREPARE阶段发起请求，容易导致存在Acceptor里面的R一直被复写，从而导致没有Proposer的ACCEPT请求被通过。解决方法可以让Proposer在接收到Timeout之后进行指数增长的等待，让其他获取到Promise的Proposer完成ACCEPT请求。
* 一个server可以同时包括Proposer, Acceptor以及Learner。
* 因为要防止single point failure, 所以不能采用单个Acceptor。
* Acceptor的数量一定要奇数，这样才能确保超过半数的Proposal对暂时无法访问到的机器的影响。
* server之间的RPC（Remote Procedure Call）不稳定，Paxos就是为了能让超过半数server live的情况下提供服务设计出来的，因此，如果超过半数的server宕机，那么整个cluster将无法提供服务。
* 只有Proposal才知道被选择的value。如果其他的server也想知道，他们也可以做一次Paxos来获取value。

**Multi-Paxos**

解决PAXOS的LiveLock问题的另一个方法就是用Multi-Paxos来进行leader election。

在Multi-Paxos中，每一个log entry都是一个Paxos实例。每个PREPARE和Accept请求中都会在log中增加一个索引，用来选择log。

Multi-Paxos也有很多问题需要解决：

* 怎么选择一个log entry？
* 怎么提升效率？
  * 用Leader来减少Proposer的livelock
  * 减少PREPARE请求
* 如何保证所有Server的数据相同？
* 客户端应该怎么与cluster交互？
* 如果configuration改变，比如增加server，怎么确保所有server都清楚？

*怎么选择 log entry*

Proposal要在本地维护一个Logs以及第一个没被Accept的索引记为index, 当客户端发送请求到Proposer后，Proposer首先对这个index 运行Paxos算法，如果这个算法返回了acceptedValue，那么就把该index和value Accept，更新index, 然后用新索引重复以上过程。如果这个算法没有返回AcceptedValue, 那么就说明client发出的请求可用，就Accept client的请求。

多台服务器可以同时对不同的客户端的请求作出处理，只要选择不同的log entries就可以了。但是状态机要求按输入顺序运行指令，所以，哪怕index较大的log entries先被Accept，上面记录的value(Command)还是不能被状态机处理。

*性能优化*

Paxos可能会有livelock，此外，Paxos两个阶段要有两个来回的RPC调用。因此，我们可以选择一个leader来当proposer，这样所有的client发送请求时都将请求发给这个proposer，这样就避免了livelock。此外，PREPARE的次数没有必要针对单个index，一次PREPARE，我们可以对整个log进行PREPARE。这样就极大的减少了RPC的数量。但是如果leader宕机，选出新leader之后，PREPARE还是要重新运行一遍的。

*Leader选举*

1. 让server ID 最大的当Leader
2. Server之间通过一个固定时长的心跳来保持联系
3. 超过心跳时长一定时间后，如果一个Server没有收到来自比他ID更高的Server的心跳，那么他就开始成为Leader。
4. 如果一个Server不是Leader，那么他会把client请求重定向到leader

*减少PREPARE次数*

PREPARE阶段的作用是

* 无视掉ID较小的Proposal
  * 让Proposal ID代表的是整个Log而不是单独的一个index，这样一个PREPARE请求就可以准备好整个log

* 找到可能存在的被Accept的value.
  * 另一个方法是PREPARE请求的结果返回的不光包括这个index的最晚ACCEPT的Proposal信息，并且，如果在这个index之后没有Accept的log entry，就返回一个noMoreAccepted的boolean，这样Proposer就知道可以skip掉之后的PREPARE了。当然如果ACCEPT请求被ignore了，那么还是要重新申请PREPARE的。

*完全复制*

在Paxos算法结束后，由于我们选择的是超过半数就确认，所以并不是所有的server都有相同的数据，而且也只有Proposer才知道哪个value才是被最终Accepted的。

解决方法是：

1. proposer在确定一个value被Accepted后，在后台不停重发accept请求直到所有Acceptor都回应这个请求。但是这种情况下，我们只能保证在这个server上新创建的log entry被复制到其他Server, 对于一些之前没能完全复制完毕就宕机的log entry我们无法通过这种方法做到同步。
2. 将已知被Accepted的log entry的Proposal ID标记为INFINITY。因为log entry已经被选定，所以标记成INFINITY也不会对entry产生影响，此外，每个server都要维护一个`firstUnchosenIndex`，用来标记第一个没有被标记为Accepted的entry。
3. Proposer将`firstUnchosenIndex`包含在ACCEPT请求中发送给Acceptor。Acceptor会在以下的情况下将一个log entry E标记为Accepted: E的index小于`firstUnchosenIndex`并且E的Proposal ID等于ACCEPT请求的Proposal ID。通过这种方法，Acceptor将在接受Accept请求的过程中ACCEPT大部分的log entry。
4. 对于上任Leader遗留的未处理的log: Acceptor将在ACCEPT请求的回复中携带它本地的`firstUnchosenIndex`，如果Proposer本地的`firstUnchosenIndex`比Acceptor的要大，那么说明Acceptor中的某些index的状态并不能通过step 3的方法同步，因此Proposer会在后台发布SUCCESS RPC用以帮助Acceptor同步。
5. SUCCESS请求包括了Acceptor本地的`firstUnchosenIndex`的index和value值，Acceptor接收到后会把该index的值更新为value，并将该index的Proposal ID变为INFINITY，之后会返回新的`firstUnchosenIndex`。Proposer会持续发送SUCCESS直到Acceptor返回的`firstUnchosenIndex`与自己本地的相同。

*客户端交互*

客户端向leader发送请求后，只有当客户端的指令被Accepted并且同步到所有服务器中，而且由leader的状态机运行完毕后，leader才会恢复客户端，并携带状态机运行所产生的结果。

如果leader宕机了，那么client有可能重复发送指令，比如，leader在状态机运行后还没发送返回结果时宕机。这种情况下，为了避免重复运行指令，客户端要发送一个unique id，leader要将这个id存在log entry中并同步到所有的server。状态机要记录每个client最近执行的指令。状态机执行指令的时候回查看指令是否执行过（通过判断ID大小），如果执行过，就会直接返回之前执行产生的结果。

*配置变更*

因为Paxos算法依赖于超过半数这个概念，因此配置变化（改变Server数量）会影响到这个判断。

Paxos的解决方法是将配置信息当做log entry储存在服务器，这样便可以保持同步，同时，维护一个α值，服务器执行第i条log entry的时候要使用`i - α`位置的配置（如果该位置没有配置，就使用之前一个的configuration），但是这样对于并发就有了限制，因为只有i被Accepted之后，我们才能Accepted `i + α`。

**Raft算法**

想让一个分布式系统达成共识，有两种途径：

1. Leader-less 所有的server的职责相同，客户端可以与所有客户交互。
2. Leader-based 在任何时刻，有且只有一个server是leader，其他的server只能接受server发送的指令。客户端只与leader交互。

Raft就是基于第二种方式。它把共识问题分解成了两个问题，leader如何分享共识，以及如何选出leader。由于leader单一，所以分享共识的过程不会有conflict，而且由于只需要leader与其他server交互，其他server之间没有交流，因此更有效率。

*Raft算法中Server的状态*

在任意时刻，每个server只能处于下列角色中的一个：

1. Leader 负责与客户端交互，并且分享共识。
2. Follower 完全被动，只接受并回复Leader发送的RPC。
3. Candidate 参与竞选leader

Raft算法用到了一个关键词Term来代表一个Leader的任期。这样就可以将时间分割成一段段的term，每个term最多只有一个Leader。每个Term由两段时间组成，第一段是竞选阶段，用来选出leader。第二段是共识操作阶段，用来让leader进行共识操作直到leader宕机。特殊情况下，如果竞选失败（多个candidate发起竞选，均不能获得超过半票），那么Term将没有Leader，此时term将直接在选举阶段结束。每个server都要维护当前的term值，即使宕机重启后，term值也应该保留下来，因此应该将Term储存起来。

*心跳和超时*

当一个集群开始运行时，所有的server都是以follower的身份开始的，作为follower，他们会等待Leader或者Candidiate向他们发送RPC请求。即使没有共识操作，leader还是要以一定的时间间隔向他的followers发送心跳请求（一般是一个空的请求）用来通知followers他还或者。如果follower超过一定的时间还没有收到leader发送的RPC请求，那么他可以认定leader宕机，这时follower将开始一轮新的leader选举。一般情况下，这个超时是100-500毫秒。

*Leader选举*

Leader选举包括两种情况，从0开始选出一个leader，以及如果现任leader宕机，选出新leader。

选举的过程包括：

* Follower首先把term的值 + 1
* Follower把自己转变为candidate参加竞选
* Candidate首先给自己投票
* Candidate不停向其他server发起投票的RPC请求，直到以下三种情况发生：
  1. 接收到超过半数的投票，这种情况下Candidate会将身份转换为Leader，并开始履行leader的责任，将心跳发送给其他server。
  2. 接收到Leader的RPC。这就说明在他得知自己的选举结果之前，已经有leader被选出，这时，他会恢复follower的状态。
  3. 无人胜选。如果不停发送投票请求却一直没收到超过半数回应，如果这个时长超过了一定值，那么就会重复一遍整个选举过程。

对于选举，我们要确保选举是可靠的，并且是可行的。为了保证选举可靠，那么每个发起投票的server在一个Term期间，只能给一个Candidate投票，因此投票过后，server必须将该投票保存在本地。因此，就不会产生两个Candidate都获得超过一半选票的情况。 为了保证可行性，即集群最终一定会选出一个leader, 对于等待投票结果的超时，我们可以采用一个时长从心跳时长T到两倍的T的随机数，这样就保证了，不同的server timeout的时间不同，从而降低了他们同时发起选举的可能。如果T选大于广播投票请求的时长的话，整个选举将很少出现无人胜选的情况。

*Log结构*

每个server将保存自己的log副本，同Multi-Paxos类似，每个log是由很多个log entry组成的，这些个log entry由index来标识。每个log entry中同时保存了command和发起command时的term。Log需要存在disk上，因为即使server宕机重启，里面的log内容不能丢失。类似于Paxos的ACCEPTED，一个log entry只有在被大多数server储存后才会被leader认为Committed，leader才能把这个command交付给状态机执行。

*共识操作*

整个共识流程如下：

* 客户端向Leader发送command
* Leader首先将command放到自己的第一个空的log slot中
* Leader将包含了command，index的AppendEntry RPC发送给所有的followers
* 如果超过半数的followers返回添加成功
  * Leader就会将该log entry标记为commited
  * 之后将command交给状态机执行，并将状态机的结果返回给客户端。
  * Leader将会给所有的followers发送后续的AppendEntry RPC告知他们该command已经被committed
  * Follower接收到committed RPC后，便可以把自己本地储存的那个Entry标记为committed，并交付给状态机运行。
* 如果follower宕机，那么leader会持续重发RPC，直到成功。
* 通常情况下，我们可以认为整个系统的表现是很好的，因为我们只需要超过半数的server储存后我们就可以认为一个entry被committed。

*Log一致*

那么Raft是怎么确保日志一致的呢？

* 对于不同server上面的log我们可以认为有以下的连贯性：
  * 对于在不同servers上面的拥有相同index和term值的log entries的command是相同的，并且在这个log entry之前的所有的log entries也都是相同的。
* 如果一个log entry被committed，那么所有在它之前的log entry必定是committed。

那么我们要怎么保证整个系统保持这个连贯性呢？为此，AppendEntry RPC要进行一致性确认。

* 每个AppendEntry的RPC都要包含前一个index位置的index和term
* Follower收到RPC请求后会查看该index位置的log entry，如果term或index不一样，那么他会拒绝这个RPC请求。
* 这个过程相当于一个递推过程，只有上一个index的情况相同，才接收下一个值，因此可以保证连贯性。

*Leader变化*

当Leader发生变化，一个新的Leader开始他的任期的时候：

* 旧的Leader可能只讲一部分的entry发送给了其他的server。
* 新的Leader并不会先进行一致性清理操作，而是直接进行共识操作。
* Source of Truth永远是当前Leader的log
* 当整个共识流程进行下去，最终，所有的server的log将和Leader的一样。
* 但是如果有多次宕机，那么不同的server上的log将会是一团乱麻。

这么看起来好像不太合理，随着宕机增多，不同的server将储存大量不同的entry，并且被选为Leader的server也许并没有接收到一些已经被前任Leader们committed的一些log entry, 因此，我们有必要对系统的安全性提出一些要求：当一个log entry被状态机运行过后，那么其他的状态机必须使用相同的entry value。也就是说，如果一个leader已经判定一个log entry被committed了，那么这个log entry必定会作为committed的log entry出现在每一个未来leader的log list上。这就要求，

* Leader不能覆盖掉他们log之中的entry。
* 只有leader log中的entry才可以被committed。
* Entries只有在被状态机处理前，必须要committed才行。

想要达成上述条件，那么我们就要对commitment的定义，以及leader选举的方式作出一些调整。

*选出最有可能包括所有committed log的Leader*

由于网络的问题，一个candidate参加选举时并不能保证确认到自己所储存的最后一个log entry是不是被上一任leader commit过，（Leader收到超过半数的回复后会commit entry，但是在leader通知follower之前，有可能存有该entry的server和leader同时宕机了，因此candidate也许没办法收集够超过半数拥有该entry的server。）所以我们要对leader选举作出一点调整，当candidate发出投票请求时，他要把自己的当前存的最大index的entry的index lastIndex_c和term lastTerm_c附加到vote请求中，这样当参与投票的server接收到vote请求后, server要将last_term_c， lastIndex_c和自己本地的lastTerm_v, lastIndex_v作比较，如果lastTerm_v > lastTerm_c 或者 lastTerm_v == lastTerm_c 同时 lastIndex_v > lastIndex_c的话，那么server会认为自己的log比candidate的老，从而拒绝给candidate投票，通过这种方式选择出来的leader，将会拥有某种意义上最完整的log（因为选举本身要收到超过半数的投票，那么就证明有超过半数的server的log没有该candidate上面的log新，也就是即使有比该candidate新的log，多出的那些个entry也一定没有被commited)。

但是这样就能保证不会漏掉commit么？一定没有被committed不能代表包括全部commit的。

现在我们来看两个例子：

例子一：

S1是term2的leader，他在将AppendEntry index4发送给S2,S3后宕机，此时，由于S5的lastTerm比S1 - S4的小，所以他不可能获得选票，因此不会当选。S4虽然Term与S1-S3相同，但是lastIndex比他们小也不会当选，因此S2，S3都有可能获得超过半数选票当选，从而可以确保Index4这个已经被committed的entry被成功保留了下来。

![Committing from current Term](https://github.com/haotianlyu/haotianlyu.github.io/blob/master/images/committing_from_current_term.jpg?raw=true)

例子二：

如果Leader想要确认commit的entry不是当前term产生的，而是由之前的term留下来的。此时S1作为Leader，在它将AppendEntry index3请求发送给S2，S3之后宕机，此时我们来看S5，S5由于某些原因，可能担任过term 3的leader，然而在它接收了一些请求后，还没来得及发送appendEntry RPC请求就宕机了，因此在他的log上面，存有一些term 3时期的log entry。在S1宕机后，如果S5参加竞选，那么由于其他Server的lastTerm还停留在Term2，拥有Term3的S5即使无法获得S1的选票，也可以获得超过半数的选票，从而导致本index 3 term2位置的已经被committed log entry被S5复写掉，这是我们不能接受的。

![Committing from earlier Term](https://github.com/haotianlyu/haotianlyu.github.io/blob/master/images/committing_from_earlier_term.jpg?raw=true)

所以，我们要更新commitment的规则：

Leader判断一个entry是不是committed不仅仅要做到该entry被储存在超过半数的server上，同时也要求，必须有一个当前term的entry被超过半数的servers储存后，才能将之前的entry确认为commit，并交付给状态机运行。也就是如果在例子二中，如果S1不commit自己term4任期内的entry 4，那么即使entry 3有超过半数的机器储存，leader也不能将他标记为committed，也就不会将他交付给虚拟机运行。从而哪怕S5当选为了leader，由于前任leader并没有把entry 3标记为committed，他就可以放心的复写掉其他机器上面的entry 3。同样的，当S1确认了entry 4 committed，那么S5由于lastTerm比超过半数的机器小，就不会当选为leader。

由此，通过更新leader election rule和commitment rule，我们确保了Raft算法的安全性。

*同步Follower的log*

新选出来的leader必须要将follower的logs与自己的同步，包括删除多余的节点，以及填充缺失的节点。

Leader将维护一个储存了所有follower nextIndex的数组。nextIndex初始值将是leader的lastIndex + 1。当leader向followers发送AppendEntry请求时，还记得AppendEntry请求要附带什么参数吗？没错，要附带当前的leader的lastIndex位置的term和index。follower收到这个AppendEntry之后，会将该位置的entry的index和term与自己的比较，如果不一样，他们会reject这个RPC，leader知道之后，会将该follower的nextIndex值减一，并将nextIndex前一个log entry的term和index重新发送给follower，直到follower找到一个相同的entry。由此，leader会将follower的nextIndex将会再开始递增。follower找到相同的entry之后就会添加/覆盖AppendEntry RPC中携带的log entry到对应位置，当覆盖的时候，会将后续所有的log entry都清空。

*旧Leader假死*

一个leader会因为网络原因与followers断开连接，这时其他的server会因为失去leader的心跳进行新的选举，此时，如果旧的leader重连回来，并重新将RPC发送给他曾经的follower的话，我们要怎么确保server知道该不该接受这个RPC呢，答案就是依靠term，无论是leader还是candidate，在发送RPC时，都应该附带自己本地的current Term值，follower/votes接收到RPC后会比较term值，如果发送方的term值更旧，那么该RPC会被拒绝，发送方会转变为follower并从新leader处更新term值。如果接收方的更旧，那么他也会转变为follower，更新term值，并正常处理该RPC请求。这样，通过竞选过程中的RPC，超过半数的server将会获得最新的term值，旧的假死leader将因为拿不到超过半数的回复，从而无法commit新的entry。

*客户端交互*

客户端发送消息给leader，如果不知道谁是leader，那么随机发给一个server，如果那个server不是leader，那么会重定向该请求给leader。leader只有在完成log，commit并将command交付给状态机运行后，才会回复状态机。如果发送给leader的请求timeout了，那么客户端将假定leader宕机，他会重新找个server发送请求，直到自己的请求得到回复。如果leader在commit后或者交付给状态机运行后宕机，这样会导致client收不到回复从而发送重复的请求。因此，与Paxos的处理方法相同，client的请求要附带unique ID，leader会将该unique ID存在log entry中，那么leader在接收到一条请求后，将检查自己的logs，如果logs有相同ID，那么就不会创建新的entry，并会将找到的对应的结果返回给客户端。

*配置更新*

和Multi-paxos一样，Raft也会有配置更新问题，server需要知道整个集群的总数从而确认自己是否达到半数。Raft处理配置更新的方式是采用两阶段的处理。第一阶段，leader会采用一种组合共识来判断一个entry是否应该commit，组合共识要求entry必须在被超过半数的旧配置server和超过半数的新配置server同时储存才能被认定为commit。同时，configuration的变化，也会被当做一个entry log，但是与一般entry log不同的是，这个entry log会立刻被状态机获取并运行（无论是否被commit）。当组合共识的entry被committed之后就会开始更新同步新的configuration的entry。当新的配置entry被committed,那么leader就将开始用新配置进行共识操作。在初始阶段，无论leader是不是使用新的配置，都可以接续工作，但是当新的配置entry被committed并且leader的配置还是旧配置的话，他就要停止当leader，转为follower等待新的拥有新配置的leader出现。

**ZAB**

Zookeeper Atomic broadcast是zookeeper用于支持崩溃恢复的原子广播协议，通过这个协议，zookeeper实现了数据一致性，并实现了一种通过主备模式的系统架构来保持各个集群中副本之间的一致性。

zookeeper的消息系统有以下特性：

* 可靠传输： 如果一个消息被一台server接受，那么它最终将送达所有服务器
* 全局有序：如果在一个server上一个事务a在事务b之前送达，那么所有的servers上都将是a比b先送达。
* 有序性：如果leader先发送消息a再发送消息b，那么a一定会排在b前面。

zookeeper在数据一致性上有如下特性：

* 顺序一致性：如果客户端将一个znode的value赋值a之后又赋值为b，那么这个过程中不会有客户端在value变为b之后仍然获取a的值。
* 原子性：zookeeper中的更新要么成功要么失败，没有中间状态，实现方法就是CAS。
* 系统视图唯一性：无论客户端连接到哪台server，都将看到唯一的系统视图，如果客户端在一个session连接中连接一个新的server，那么它看到的系统视图一定不会比之前看到的系统视图旧。
* 持久性：数据都是被持久化到server上的，如果server宕机重启，那么数据不会丢失。
* 时效性：如果一个server的系统视图落后于其他server太多，那么zookeeper会关闭该server。

*zookeeper节点状态*

zookeeper中节点有以下四种状态：

1. Looking 寻找Leader的状态，当服务器处于该状态时会认为集群中没有Leader，从而需要进入Leader选举状态，集群初始化时，默认是这个状态。
2. Following 处理客户端的非事务操作，将把事务操作转发给服务器，参与Leader选举的投票以及对于事务请求Proposal的投票
3. Leading Leader状态，事务请求的唯一调度者和矗立着，保证集群实物处理的顺序性，集群内部各个server的调度者，负责管理follower以及数据同步。
4. Observing Observer不参与事务操作以及投票，可以在不影响集群事务处理能力的基础上提升集群的非事务处理能力。

每个server在leader election时，会发送如下的信息：

* epoch-logicClock 每个server会维护一个自增的证书，用来表示这是这个server发起的第几轮投票
* state 当前server的状态
* serverID 也叫做myid
* self_zxid 当前server上所保存的数据最大的ZXID
* vote_id 当前server将选票投给那个server的myid
* vote_zxid 当前server将选票给的那个server保存的最大的ZXID

投票过程

1. 自增选举轮次：zookeeper规定所有有效的投票都必须在同一个轮次中，每个服务器开始新一轮投票时，会先对自己维护的logicClock进行自增操作。
2. 初始化选票： 每个server在广播自己的选票钱，会将自己的投票箱清空，投票箱里记录了收到的所有的选票，这个选票会记录每个server投给任意server的选票，但是投票箱只会维护每个投票server投出的最后一票。
3. 发送初始化选票：每个server在开始阶段都是把票投给自己。
4. 接收外部投票：接收外部投票的过程中，要确认自己与其他服务器是否保持有效连接，如果一直无法获取有效投票，且仍保留有效连接，那么将再次发送自己的投票。
5. 判断选举轮次：server根据投票信息中的logicClock来判断如何对投票做出处理，有以下的三种方式：
   * 外部投票的logic Clock大于自己本地的logic Clock，这种情况下说明自己的投票轮次落后了，server会更新自己的logic Clock并根据投票箱内投票的情况更新自己的选票，并重新广播自己的选票。
   * 外部的logic Clock比自己本地的logic Clock小，那么server会直接忽略该投票。
   * 当外部的logic Clock和自己本地的logic Clock一样大，那么就进行选票PK。
6. 选票PK是基于vote_id，vote_zxid
   * 首先如果外部投票的logic Clock大，首先应该更新logic Clock
   * 如果logic Clock一致了，那么对比外部投票的vote_zxid与本地的vote_zxid, 如果外部的大，就将自己的投票中的zxid和serverID更新，并广播出去。之后将外部投票和自己的投票放入票箱。
   * 如果vote_zxid都一致，那么就比较serverID,如果外部的大，就将自己的选票中的serverID更新并广播出去。之后将外部投票和自己的投票放入票箱。
7. 统计选票，如果有超过半数的服务器的投票和自己的投票相同，那么就终止投票，否则就持续投票直到达成条件。
8. 当投票结束后，服务器要更新自身状态，如果超过半数的票投给自己，就将自己的state更新为leading，否则将自己的state更新为follower。
9. Leader要发起并维护与follower之间的心跳。
10. 如果有follower断开连接后重连，会进入looking状态发起投票，当leader收到该follower的投票后，会将自己的state leading和选票返回给follower，而其他的follower会将自己的状态follower和选票发送给looking状态的follower，在follower验证过后，会重新回到follow状态。

*ZXID*

Zookeeper采用全剧递增的事务ID来标识，所有的Proposal都会被加上ZXID，这个ZXID是保证事务的顺序一致性的关键。ZXID由64bits组成，前32位被称为epoch （纪元），后32位表示 xid 事务标识。也就是事务的ZXID越大，那么它储存的数据越新。 每个leader都会有unique的epoch，类似于raft中的term ID，选举结束产生新的leader的话leader会将整个集群中最大ZXID的epoch + 1，作为自己的epoch，并将该值更新给所有的server。

*server ID*

搭建zookeeper集群的时候我们会在myid中给每个server设置一个编号，这个就是server ID，他可以作为一个权重用于zookeeper算法。

*zab数据一致性*

zab协议设计了两种工作模式，zookeeper通过在这两种模式之间切换，保证数据一致性。

1. 原子广播模式
   1. 从客户端收到的写请求后，leader生成一个事务，并给这个事务生成一个unique的ZXID。
   2. Leader将带有ZXID的proposal发送给每个server的FIFO队列。
   3. FIFO队列取出头上的proposal发送给follower
   4. 当follow收到proposal之后，将proposal写到自己的disk上，写入成功后，会向leader返回一个ACK。
   5. leader收到超过半数的ACK消息后，leader会进行commit请求，会再次给FIFO发送commit请求。
   6. follower用FIFO队列中收到commit请求时，会判断该事务的ZXID是不是比自己的历史队列中任何事务的ZXID都小，如果是，那么就commit，否则将等待直到比他更小的事务完成commit。
   7. leader在将commit发送给followers之后会运行处理结果并将结果返回给客户端。
   8. FIFO队列使用TCP协议，因为TCP协议是有序传输的，即数据发送和数据送达的顺序严格一致，即消息m被送达当且仅当m前发送的消息都被送达。此外，FIFO一旦关系，将不再接受消息。
2. 恢复模式
   1. 当leader宕机后，集群进入选举阶段，开始选举出新leader
   2. 进入发现阶段，followers会与新leader进行沟通，主动将自己本地的最大的zxid发送给leader，leader会将follower的zxid和自身的zxid间所有的被commit过的消息同步给follower[包括proposal和commit两步]。新的server成为leader之后要判断自身还未commit的消息是否存在于大多数服务器中，从而决定是否要将其commit。之后leader会得到自身所包含的被commit过的消息的ZXID的最小值和最大值。Follower向leader发送的消息包括自己本地commit过的最大zxid以及还未被commit的所有消息。leader会根据这些信息对follower作出以下操作：
      * 如果follwoer的最大ZXID与leader的最大ZXID相同，那么该follower是与leader完全同步的，不需要同步任何其他数据。
      * 如果follower的最大ZXID在leader的有效ZXID之间，那么leader会发送TRUNC命令要求follower删除掉未被commit过的消息中比follower最大ZXID更大的消息。
   3. 当数据同步结束后，leader会向followers发送NEWLEADER命令，follower在收到命令后会返回ACK，当leader收到超过半数的ACK之后，会再次向followers发送UPTODATE命令，follower接收到命令后就可以开始对外提供服务了。

